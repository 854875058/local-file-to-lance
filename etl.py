# -*- coding: utf-8 -*-
"""ETLï¼šå†…å®¹æå–ã€ç®¡é“å¤„ç†ã€æ‰¹é‡/SFTP ä»»åŠ¡"""

import os
import uuid
import time
import logging
import shutil
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

import boto3
import pandas as pd
from PIL import Image
import docx
import pypdf
from pdf2image import convert_from_path
from pptx import Presentation
import paramiko

from config import (
    S3_CONFIG,
    TEMP_DIR,
    EXTRACT_DIR,
    CONTENT_EXTS,
    IMAGE_EXTS,
    ARCHIVE_EXTS,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    MAX_FILE_SIZE_MB,
)
from database import (
    calculate_file_hash,
    check_file_exists,
    register_file,
    insert_task_stat,
    delete_file_from_registry,
    insert_file_entities,
)
from models_loader import get_text_splitter

logger = logging.getLogger(__name__)

# S3 å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
_s3_client = None

def _sanitize_filename(name: str) -> str:
    # S3 key é‡Œé¿å…å‡ºç°è·¯å¾„åˆ†éš”ç¬¦
    return (name or "").replace("\\", "_").replace("/", "_")


def _category_for_ext(ext: str) -> str:
    e = (ext or "").lower()
    if e in IMAGE_EXTS:
        return "image"
    if e in ["mp3", "wav", "m4a", "flac", "ogg"]:
        return "audio"
    if e in ["mp4", "webm", "mov", "avi", "mkv"]:
        return "video"
    if e in ["pdf", "docx", "pptx", "txt", "md", "csv", "xlsx", "xls", "json", "log", "sql", "xml", "yaml", "ini", "py", "js", "sh"]:
        return "text"
    if e in ARCHIVE_EXTS:
        return "archive"
    return "other"


def get_s3_client():
    global _s3_client
    if _s3_client is None:
        try:
            _s3_client = boto3.client(
                "s3",
                endpoint_url=S3_CONFIG["endpoint_url"],
                aws_access_key_id=S3_CONFIG["access_key_id"],
                aws_secret_access_key=S3_CONFIG["secret_access_key"],
            )
            # åŸå§‹æ–‡ä»¶æ¡¶ & LanceDB æ¡¶ï¼ˆLanceDB æ¡¶ä¸»è¦ç»™ lancedb è‡ªå·±ç”¨ï¼Œè¿™é‡Œåªç¡®ä¿å­˜åœ¨ï¼‰
            for b in [S3_CONFIG["raw_bucket"], S3_CONFIG["lance_bucket"]]:
                try:
                    _s3_client.create_bucket(Bucket=b)
                except Exception:
                    pass
        except Exception:
            _s3_client = False
    return _s3_client if _s3_client else None


def extract_content(path, ext, models):
    """å…¨èƒ½å†…å®¹æå–ï¼šæ–‡æœ¬ã€æ–‡æ¡£ã€è¡¨æ ¼ã€éŸ³è§†é¢‘"""
    content = ""
    msg = ""

    try:
        if ext in ["mp3", "wav", "m4a", "mp4", "avi", "mov", "mkv", "flac"]:
            result = models["whisper"].transcribe(path)
            content = result.get("text", "")
            msg = "è¯­éŸ³è½¬å½•å®Œæˆ"

        elif ext in ["txt", "md", "py", "json", "log", "sh", "js", "java", "sql", "xml", "yaml", "ini"]:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()

        elif ext == "docx":
            content = "\n".join([p.text for p in docx.Document(path).paragraphs])
        elif ext == "pdf":
            content = "\n".join(
                [p.extract_text() or "" for p in pypdf.PdfReader(path).pages]
            )
        elif ext == "pptx":
            prs = Presentation(path)
            content = "\n".join(
                shape.text
                for slide in prs.slides
                for shape in slide.shapes
                if hasattr(shape, "text")
            )
        elif ext == "csv":
            content = pd.read_csv(path).to_string()
        elif ext in ["xlsx", "xls"]:
            content = pd.read_excel(path).to_string()
        elif ext == "parquet":
            content = pd.read_parquet(path).to_string()
    except Exception as e:
        logger.error("æå–å¤±è´¥ %s: %s", ext, e)
        msg = str(e)

    return content, msg


def process_pipeline(local_path, original_filename, models, tbl_text, tbl_image, tbl_files):
    if original_filename is None:
        original_filename = os.path.basename(local_path)
    ext = original_filename.split(".")[-1].lower()

    overwrite = False
    try:
        f_hash = calculate_file_hash(local_path)
        overwrite = check_file_exists(f_hash)
        # SQLite ç™»è®°ï¼ˆé‡å¤åˆ™å¿½ç•¥ï¼‰
        register_file(f_hash, original_filename, os.path.getsize(local_path))
    except Exception:
        # f_hash ä»å¯èƒ½ç”¨äºåç»­æµç¨‹ï¼›è‹¥å¼‚å¸¸ï¼Œåé¢ä¼šå†ç®—ä¸€æ¬¡
        pass

    # å‹ç¼©åŒ…
    if ext in ARCHIVE_EXTS:
        import zipfile
        import tarfile
        import shutil

        try:
            extract_folder = os.path.join(EXTRACT_DIR, str(uuid.uuid4()))
            os.makedirs(extract_folder)
            if ext == "zip":
                with zipfile.ZipFile(local_path, "r") as z:
                    z.extractall(extract_folder)
            else:
                with tarfile.open(local_path, "r") as t:
                    t.extractall(extract_folder)

            sub_files = []
            for root, _, files in os.walk(extract_folder):
                for f in files:
                    if not f.startswith("."):
                        sub_files.append((os.path.join(root, f), f))

            total = 0
            for p, n in sub_files:
                res = process_pipeline(p, n, models, tbl_text, tbl_image, tbl_files)
                if res["success"]:
                    total += res["count"]
            shutil.rmtree(extract_folder)
            return {"success": True, "msg": f"è§£å‹å…¥åº“ {total} æ–‡ä»¶", "count": total, "status": "ok"}
        except Exception as e:
            return {"success": False, "msg": str(e), "count": 0, "status": "error"}

    # å•æ–‡ä»¶
    try:
        f_hash = calculate_file_hash(local_path)
        if not f_hash:
            return {"success": False, "msg": "æ–‡ä»¶hashè®¡ç®—å¤±è´¥", "count": 0, "status": "error"}

        s3_client = get_s3_client()
        s3_uri = f"local://{original_filename}"
        if s3_client:
            try:
                safe_name = _sanitize_filename(original_filename)
                cat = _category_for_ext(ext)
                # S3 é‡Œ"ç›®å½•"æœ¬è´¨æ˜¯ key å‰ç¼€ï¼ŒæŒ‰æ—¥æœŸ + ç±»å‹åˆ†ç»„æ›´å¥½ç®¡ç†
                today = datetime.now().strftime("%Y/%m/%d")
                key = f"raw/{today}/{cat}/{uuid.uuid4().hex[:8]}_{safe_name}"
                s3_client.upload_file(local_path, S3_CONFIG["raw_bucket"], key)
                s3_uri = f"s3://{S3_CONFIG['raw_bucket']}/{key}"
            except Exception as e:
                logger.warning(f"S3ä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°URI: {e}")

        processed = False

        # è¦†ç›–å¼é‡è·‘ï¼šåŒä¸€ file_hash å…ˆåˆ æ—§è®°å½•ï¼Œå†å†™æ–°è®°å½•ï¼ˆä¿è¯é¢„è§ˆ/æ£€ç´¢ä¸€è‡´ï¼‰
        # æ³¨æ„ï¼šä¸ºäº†é¿å…åˆ é™¤åå†™å…¥å¤±è´¥å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼Œæˆ‘ä»¬å…ˆå‡†å¤‡å¥½æ‰€æœ‰æ•°æ®å†åˆ é™¤
        if overwrite:
            logger.info(f"æ£€æµ‹åˆ°é‡å¤æ–‡ä»¶ï¼Œå°†è¦†ç›–: {original_filename}, hash={f_hash}")

        # 1) åŸå§‹æ–‡ä»¶å…¥ LanceDBï¼ˆç”¨äºæ•´æ–‡ä»¶é¢„è§ˆ/ä¸‹è½½ï¼‰
        file_bytes = None
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜
            file_size = os.path.getsize(local_path)
            max_file_size = MAX_FILE_SIZE_MB * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚

            if file_size > max_file_size:
                logger.warning(f"æ–‡ä»¶è¿‡å¤§ ({file_size / 1024 / 1024:.2f}MB)ï¼Œè·³è¿‡å­˜å‚¨åˆ° files è¡¨: {original_filename}")
                # å¤§æ–‡ä»¶åªå­˜å‚¨å…ƒæ•°æ®ï¼Œä¸å­˜å‚¨ bytes
                file_row = {
                    "file_hash": f_hash,
                    "doc_name": original_filename,
                    "doc_type": ext,
                    "source_uri": s3_uri,
                    "file_bytes": b"",  # ç©º bytes
                    "text_full": "",
                }
            else:
                with open(local_path, "rb") as rf:
                    file_bytes = rf.read()

                if not file_bytes:
                    logger.error(f"æ–‡ä»¶è¯»å–ä¸ºç©º: {original_filename}")
                    return {"success": False, "msg": "æ–‡ä»¶è¯»å–ä¸ºç©º", "count": 0, "status": "error"}

                file_row = {
                    "file_hash": f_hash,
                    "doc_name": original_filename,
                    "doc_type": ext,
                    "source_uri": s3_uri,
                    "file_bytes": file_bytes,
                    "text_full": "",
                }

            # å¦‚æœæ˜¯è¦†ç›–æ¨¡å¼ï¼Œå…ˆåˆ é™¤æ—§çš„ files è¡¨è®°å½•
            if overwrite:
                try:
                    # è½¬ä¹‰å•å¼•å·é¿å… SQL æ³¨å…¥
                    safe_hash = f_hash.replace("'", "''")
                    tbl_files.delete(f"file_hash = '{safe_hash}'")
                    logger.info(f"å·²åˆ é™¤æ—§çš„ files è¡¨è®°å½•: hash={f_hash}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤æ—§ files è¡¨è®°å½•å¤±è´¥ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")

            # å‡†å¤‡å¹¶å†™å…¥ files è¡¨æ•°æ®
            tbl_files.add([file_row])
            logger.info(f"files è¡¨å†™å…¥æˆåŠŸ: {original_filename}, hash={f_hash}, size={file_size} bytes")
        except Exception as e:
            logger.error(f"files è¡¨å†™å…¥å¤±è´¥: {e}, file={original_filename}, hash={f_hash}")
            import traceback
            logger.error(traceback.format_exc())
            # å¦‚æœ files è¡¨å†™å…¥å¤±è´¥ï¼Œè¿”å›é”™è¯¯è€Œä¸æ˜¯ç»§ç»­å¤„ç†
            return {"success": False, "msg": f"filesè¡¨å†™å…¥å¤±è´¥: {str(e)}", "count": 0, "status": "error"}

        # 2) å‘é‡åŒ–å…¥åº“ï¼ˆç”¨äºæ£€ç´¢ï¼‰
        if ext in CONTENT_EXTS:
            content, msg = extract_content(local_path, ext, models)
            if content and content.strip():
                # å¦‚æœæ˜¯è¦†ç›–æ¨¡å¼ï¼Œå…ˆåˆ é™¤æ—§çš„ text è¡¨è®°å½•
                if overwrite:
                    try:
                        # è½¬ä¹‰å•å¼•å·é¿å… SQL æ³¨å…¥
                        safe_hash = f_hash.replace("'", "''")
                        tbl_text.delete(f"file_hash = '{safe_hash}'")
                        logger.info(f"å·²åˆ é™¤æ—§çš„ text_chunks è¡¨è®°å½•: hash={f_hash}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤æ—§ text_chunks è¡¨è®°å½•å¤±è´¥: {e}")

                splitter = get_text_splitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
                chunks = splitter.split_text(content)
                if chunks:
                    vecs = models["text"].encode(chunks)
                    data = [
                        {
                            "id": str(uuid.uuid4()),
                            "vector": v,
                            "text": c,
                            "source_uri": s3_uri,
                            "doc_name": original_filename,
                            "doc_type": ext,
                            "file_hash": f_hash,  # ç›´æ¥å†™å…¥ï¼Œè¡¨ä¸€å®šæœ‰æ­¤åˆ—
                        }
                        for c, v in zip(chunks, vecs)
                    ]
                    tbl_text.add(data)
                    logger.info(f"text_chunks è¡¨å†™å…¥æˆåŠŸ: {len(chunks)} ä¸ªåˆ‡ç‰‡, hash={f_hash}")
                    # åŒæ­¥å…¨æ–‡åˆ° files è¡¨ï¼ˆä¾¿äº"æ•´ä»½æ–‡æ¡£"é¢„è§ˆï¼‰
                    # è¿™é‡Œç”¨ updateï¼ˆè‹¥ç‰ˆæœ¬ä¸æ”¯æŒåˆ™å¿½ç•¥ï¼Œä»å¯ä¸‹è½½åŸä»¶ï¼‰
                    try:
                        safe_hash = f_hash.replace("'", "''")
                        tbl_files.where(f"file_hash = '{safe_hash}'").update({"text_full": content})
                        logger.info(f"files è¡¨ text_full æ›´æ–°æˆåŠŸ: hash={f_hash}")
                    except Exception as e:
                        logger.warning(f"files è¡¨ text_full æ›´æ–°å¤±è´¥: {e}")
                    processed = True

        if ext in IMAGE_EXTS:
            try:
                # å¦‚æœæ˜¯è¦†ç›–æ¨¡å¼ï¼Œå…ˆåˆ é™¤æ—§çš„ image è¡¨è®°å½•
                if overwrite:
                    try:
                        safe_hash = f_hash.replace("'", "''")
                        tbl_image.delete(f"file_hash = '{safe_hash}'")
                        logger.info(f"å·²åˆ é™¤æ—§çš„ image_chunks è¡¨è®°å½•: hash={f_hash}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤æ—§ image_chunks è¡¨è®°å½•å¤±è´¥: {e}")

                img = Image.open(local_path)
                vec = models["clip_vision"].encode(img)
                row = {
                    "id": str(uuid.uuid4()),
                    "vector": vec,
                    "source_uri": s3_uri,
                    "doc_name": original_filename,
                    "meta_info": "image_file",
                    "file_hash": f_hash,  # ç›´æ¥å†™å…¥ï¼Œè¡¨ä¸€å®šæœ‰æ­¤åˆ—
                }
                tbl_image.add([row])
                logger.info(f"image_chunks è¡¨å†™å…¥æˆåŠŸ: {original_filename}, hash={f_hash}")
                processed = True
            except Exception as e:
                logger.warning(f"å›¾åƒå‘é‡åŒ–å¤±è´¥: {e}")

        if ext == "pdf":
            try:
                # å¦‚æœæ˜¯è¦†ç›–æ¨¡å¼ï¼Œå…ˆåˆ é™¤æ—§çš„ PDF å›¾åƒè®°å½•
                if overwrite:
                    try:
                        safe_hash = f_hash.replace("'", "''")
                        tbl_image.delete(f"file_hash = '{safe_hash}'")
                        logger.info(f"å·²åˆ é™¤æ—§çš„ PDF å›¾åƒè®°å½•: hash={f_hash}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤æ—§ PDF å›¾åƒè®°å½•å¤±è´¥: {e}")

                images = convert_from_path(local_path)
                if images:
                    vecs = models["clip_vision"].encode(images)
                    data = [
                        {
                            "id": str(uuid.uuid4()),
                            "vector": v,
                            "source_uri": s3_uri,
                            "doc_name": original_filename,
                            "meta_info": f"Page {i+1}",
                            "file_hash": f_hash,  # ç›´æ¥å†™å…¥ï¼Œè¡¨ä¸€å®šæœ‰æ­¤åˆ—
                        }
                        for i, v in enumerate(vecs)
                    ]
                    tbl_image.add(data)
                    logger.info(f"PDF å›¾åƒå‘é‡åŒ–æˆåŠŸ: {len(images)} é¡µ, hash={f_hash}")
                    processed = True
            except Exception as e:
                logger.warning(f"PDF å›¾åƒå‘é‡åŒ–å¤±è´¥: {e}")

        if processed:
            # æ–¹å¼Bï¼šä¸è½æœ¬åœ°é¢„è§ˆç›®å½•ï¼ŒåŸå§‹æ–‡ä»¶å·²å†™å…¥ LanceDB `files` è¡¨
            return {"success": True, "msg": ("è¦†ç›–OK" if overwrite else "OK"), "count": 1, "status": "ok"}
        return {"success": False, "msg": "Skipped", "count": 0, "status": "skipped"}
    except Exception as e:
        logger.exception("process_pipeline error: %s", e)
        return {"success": False, "msg": str(e), "count": 0, "status": "error"}


def delete_file_by_hash(file_hash, tbl_text, tbl_image, tbl_files):
    """åˆ é™¤æ–‡ä»¶çš„æ‰€æœ‰æ•°æ®ï¼šfile_registry + text_chunks + image_chunks + files å››å¼ è¡¨"""
    safe_hash = file_hash.replace("'", "''")
    errors = []
    for tbl, name in [(tbl_text, 'text_chunks'), (tbl_image, 'image_chunks'), (tbl_files, 'files')]:
        try:
            tbl.delete(f"file_hash = '{safe_hash}'")
        except Exception as e:
            errors.append(f"{name}: {e}")
            logger.warning(f"åˆ é™¤ {name} è¡¨è®°å½•å¤±è´¥: {e}")
    if not delete_file_from_registry(file_hash):
        errors.append("file_registry: åˆ é™¤å¤±è´¥")
    if errors:
        logger.warning(f"åˆ é™¤æ–‡ä»¶ {file_hash} éƒ¨åˆ†å¤±è´¥: {errors}")
    return len(errors) == 0


def extract_entities_llm(text, file_hash):
    """è°ƒç”¨ DeepSeek API ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“ï¼Œå†™å…¥ file_entities è¡¨ã€‚å¤±è´¥ä¸é˜»å¡ä¸»æµç¨‹ã€‚"""
    try:
        from config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, DEEPSEEK_MODEL
        import json
        import urllib.request

        if not DEEPSEEK_API_KEY or not text or not text.strip():
            return

        # æˆªå–å‰ 3000 å­—ç¬¦é¿å… token è¿‡é•¿
        snippet = text[:3000]
        prompt = (
            "è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–å…³é”®å®ä½“ï¼ˆäººåã€åœ°åã€ç»„ç»‡ã€æŠ€æœ¯æœ¯è¯­ç­‰ï¼‰ï¼Œ"
            "ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« name å’Œ type ä¸¤ä¸ªå­—æ®µã€‚"
            "åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚\n\n"
            f"æ–‡æœ¬ï¼š{snippet}"
        )

        payload = json.dumps({
            "model": DEEPSEEK_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,
        }).encode("utf-8")

        req = urllib.request.Request(
            f"{DEEPSEEK_BASE_URL}/v1/chat/completions",
            data=payload,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            },
        )
        with urllib.request.urlopen(req, timeout=30) as resp:
            result = json.loads(resp.read().decode("utf-8"))

        content = result["choices"][0]["message"]["content"].strip()
        # å°è¯•æå– JSON æ•°ç»„
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        entities_raw = json.loads(content)
        if not isinstance(entities_raw, list):
            return

        entities = []
        for e in entities_raw:
            name = (e.get("name") or "").strip()
            etype = (e.get("type") or "").strip()
            if name:
                entities.append((name, etype))

        if entities:
            insert_file_entities(file_hash, entities)
            logger.info(f"å®ä½“æŠ½å–å®Œæˆ: hash={file_hash}, å…± {len(entities)} ä¸ªå®ä½“")
    except Exception as e:
        logger.warning(f"å®ä½“æŠ½å–å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")


def batch_process_files(files, models, tbl_text, tbl_image, tbl_files, progress_callback=None):
    start = time.time()
    total = len(files)
    temp_files = []  # è®°å½•ä¸´æ—¶æ–‡ä»¶ï¼Œç”¨äºæ¸…ç†
    results = []  # æ”¶é›†æ‰€æœ‰ç»“æœ

    def process_one(f):
        tp = os.path.join(TEMP_DIR, f"{uuid.uuid4().hex[:8]}_{f.name}")  # æ·»åŠ éšæœºå‰ç¼€é¿å…æ–‡ä»¶åå†²çª
        temp_files.append(tp)
        try:
            with open(tp, "wb") as w:
                w.write(f.getbuffer())
            return process_pipeline(tp, f.name, models, tbl_text, tbl_image, tbl_files)
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {f.name}, {e}")
            return {"success": False, "msg": str(e), "count": 0, "status": "error"}

    try:
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(process_one, f): f for f in files}
            for i, future in enumerate(as_completed(futures)):
                try:
                    res = future.result()
                    results.append(res)
                    if progress_callback:
                        progress_callback(i + 1, total, res["msg"])
                except Exception as e:
                    logger.error(f"è·å–ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
                    results.append({"success": False, "msg": str(e), "count": 0, "status": "error"})
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for tp in temp_files:
            try:
                if os.path.exists(tp):
                    os.remove(tp)
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {tp}, {e}")

    # ç»Ÿè®¡ç»“æœ
    succ = sum(r["count"] for r in results if r["status"] == "ok")
    skip = sum(1 for r in results if r["status"] == "skipped")
    dur = time.time() - start
    insert_task_stat("batch", total, succ, dur)
    return succ, skip, dur


def batch_process_local_files(file_paths, models, tbl_text, tbl_image, tbl_files, progress_callback=None):
    """å¤„ç†æœ¬åœ°æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆNiceGUI ç­‰é Streamlit å‰ç«¯ä½¿ç”¨ï¼‰ã€‚
    file_paths: list of (local_path, original_filename) å…ƒç»„
    è¿”å›: (succ, skip, dur, skipped_names)
    """
    start = time.time()
    total = len(file_paths)
    results = []
    skipped_names = []

    def process_one(item):
        local_path, name = item
        try:
            res = process_pipeline(local_path, name, models, tbl_text, tbl_image, tbl_files)
            # å¼‚æ­¥å®ä½“æŠ½å–ï¼ˆæˆåŠŸå…¥åº“çš„æ–‡æœ¬æ–‡ä»¶ï¼‰
            if res.get("status") == "ok":
                try:
                    ext = name.split(".")[-1].lower()
                    if ext in CONTENT_EXTS:
                        content, _ = extract_content(local_path, ext, models)
                        if content and content.strip():
                            f_hash = calculate_file_hash(local_path)
                            extract_entities_llm(content, f_hash)
                except Exception:
                    pass
            return res, name
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {name}, {e}")
            return {"success": False, "msg": str(e), "count": 0, "status": "error"}, name

    try:
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(process_one, item): item for item in file_paths}
            for i, future in enumerate(as_completed(futures)):
                try:
                    res, name = future.result()
                    results.append(res)
                    if res.get("status") == "skipped":
                        skipped_names.append(name)
                    if progress_callback:
                        progress_callback(i + 1, total, res["msg"])
                except Exception as e:
                    logger.error(f"è·å–ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
                    results.append({"success": False, "msg": str(e), "count": 0, "status": "error"})
    finally:
        pass  # æœ¬åœ°è·¯å¾„ç”±è°ƒç”¨æ–¹ç®¡ç†æ¸…ç†

    succ = sum(r["count"] for r in results if r["status"] == "ok")
    skip = sum(1 for r in results if r["status"] == "skipped")
    dur = time.time() - start
    insert_task_stat("batch", total, succ, dur)
    return succ, skip, dur, skipped_names


def sftp_task(host, port, user, password, path, models, tbl_text, tbl_image, tbl_files, progress_callback=None):
    logs = []
    skipped_names = []
    tr = None
    try:
        tr = paramiko.Transport((host, int(port)))
        tr.connect(username=user, password=password)
        sftp = paramiko.SFTPClient.from_transport(tr)
        files = sftp.listdir(path)
        logs.append(f"ğŸ”— æ‰«æåˆ° {len(files)} ä¸ªæ–‡ä»¶")

        local_fs = []
        total_files = 0
        for f in files:
            if f.startswith("."):
                continue
            total_files += 1
            try:
                local_path = os.path.join(TEMP_DIR, f)
                sftp.get(path.rstrip("/") + "/" + f, local_path)
                local_fs.append((local_path, f))
                if progress_callback:
                    progress_callback(len(local_fs), total_files, f"ä¸‹è½½: {f}")
            except Exception:
                logs.append(f"âš ï¸ ä¸‹è½½å¤±è´¥ {f}")

        cnt = 0
        for i, (local_path, name) in enumerate(local_fs):
            res = process_pipeline(local_path, name, models, tbl_text, tbl_image, tbl_files)
            if res["status"] == "ok":
                cnt += res["count"]
                # å¼‚æ­¥å®ä½“æŠ½å–
                try:
                    ext = name.split(".")[-1].lower()
                    if ext in CONTENT_EXTS:
                        content, _ = extract_content(local_path, ext, models)
                        if content and content.strip():
                            f_hash = calculate_file_hash(local_path)
                            extract_entities_llm(content, f_hash)
                except Exception:
                    pass
            elif res["status"] == "skipped":
                skipped_names.append(name)
            if progress_callback:
                progress_callback(i + 1, len(local_fs), f"å¤„ç†: {name}")
            if os.path.exists(local_path):
                os.remove(local_path)
        logs.append(f"ğŸ‰ å…¥åº“ {cnt} æ¡")
        if skipped_names:
            logs.append(f"â­ï¸ è·³è¿‡ {len(skipped_names)} ä¸ªæ–‡ä»¶: {', '.join(skipped_names)}")
    except Exception as e:
        logs.append(f"ğŸ”´ {e}")
    finally:
        if tr:
            tr.close()
    return logs, skipped_names
