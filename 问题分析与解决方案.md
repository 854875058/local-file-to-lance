# 问题分析与解决方案

## 📋 问题描述

检索文件时出现错误：**"⚠️ files 表未找到原始文件（可重新接入该文件后再检索）"**

## 🔍 根本原因分析

### 您的架构设计（正确理解）

1. **原始文件存储**：上传的文件存到 S3 的 `raw_bucket`（原始文件桶）
2. **LanceDB 存储**：
   - `text_chunks` / `image_chunks` 表：存向量和元数据（包含 `file_hash` 和 `source_uri`）
   - `files` 表：存原始文件的 **bytes** 和元数据（用于预览和下载）
3. **检索流程**：
   - 在 `text_chunks` / `image_chunks` 表中检索，得到 `file_hash`
   - 通过 `file_hash` 去 `files` 表查找原始文件
   - 从 `files` 表的 `file_bytes` 字段读取原始文件进行预览/下载

### 问题根源

#### 问题1：files 表写入失败但不中断流程（已修复）

**原代码逻辑：**
```python
# etl.py:199-218（旧版本）
try:
    # 写入 files 表
    tbl_files.add([file_row])
except Exception as e:
    logger.warning("files 表写入失败: %s", e)  # ← 只是警告，继续执行

# 后续继续写入 text/image 表
if ext in CONTENT_EXTS:
    tbl_text.add(data)  # ← 这里成功了，但 files 表没有记录
```

**结果**：
- `text_chunks` / `image_chunks` 表有数据（检索能找到）
- `files` 表没有对应记录（预览时找不到）
- 导致错误："未找到原始文件"

#### 问题2：覆盖模式可能导致数据丢失

**原代码逻辑：**
```python
# etl.py:192-197（旧版本）
if overwrite:
    for t in (tbl_files, tbl_text, tbl_image):
        t.delete(f"file_hash = '{f_hash}'")  # ← 先删除旧数据

# 然后写入新数据
tbl_files.add([file_row])  # ← 如果这里失败，旧数据已经被删了！
```

**问题**：如果删除成功但写入失败，数据就永久丢失了。

#### 问题3：file_hash 可能为 None 或空

如果文件读取失败，`file_hash` 可能是 None，导致查询失败。

#### 问题4：LanceDB 写入可能失败的原因

1. **S3 连接问题**：LanceDB 存储在 S3 上，网络问题可能导致写入失败
2. **权限问题**：S3 bucket 权限配置不正确
3. **大文件问题**：文件过大导致内存不足或超时
4. **并发冲突**：多个进程同时写入同一个表

## ✅ 已实施的修复

### 1. 增强错误处理和日志

**修改位置：** `etl.py:199-233`

```python
# 1) 原始文件入 LanceDB（用于整文件预览/下载）
file_bytes = None
try:
    with open(local_path, "rb") as rf:
        file_bytes = rf.read()

    if not file_bytes:
        logger.error(f"文件读取为空: {original_filename}")
        return {"success": False, "msg": "文件读取为空", "count": 0, "status": "error"}

    # 如果是覆盖模式，先删除旧的 files 表记录
    if overwrite:
        try:
            tbl_files.delete(f"file_hash = '{f_hash}'")
            logger.info(f"已删除旧的 files 表记录: hash={f_hash}")
        except Exception as e:
            logger.warning(f"删除旧 files 表记录失败（可能不存在）: {e}")

    # 准备并写入 files 表数据
    file_row = {
        "file_hash": f_hash,
        "doc_name": original_filename,
        "doc_type": ext,
        "source_uri": s3_uri,
        "file_bytes": file_bytes,
        "text_full": "",
    }
    tbl_files.add([file_row])
    logger.info(f"files 表写入成功: {original_filename}, hash={f_hash}, size={len(file_bytes)} bytes")
except Exception as e:
    logger.error(f"files 表写入失败: {e}, file={original_filename}, hash={f_hash}")
    import traceback
    logger.error(traceback.format_exc())
    # 如果 files 表写入失败，返回错误而不是继续处理
    return {"success": False, "msg": f"files表写入失败: {str(e)}", "count": 0, "status": "error"}
```

**改进点：**
- ✅ files 表写入失败时立即返回错误，不再继续处理
- ✅ 添加详细的日志记录（文件名、hash、大小）
- ✅ 记录完整的异常堆栈信息
- ✅ 检查文件是否为空

### 2. 改进覆盖模式逻辑

**修改位置：** `etl.py:235-272`（text 表）、`etl.py:274-320`（image 表）

```python
# 如果是覆盖模式，先删除旧的 text 表记录
if overwrite:
    try:
        tbl_text.delete(f"file_hash = '{f_hash}'")
        logger.info(f"已删除旧的 text_chunks 表记录: hash={f_hash}")
    except Exception as e:
        logger.warning(f"删除旧 text_chunks 表记录失败: {e}")

# 然后写入新数据
tbl_text.add(data)
logger.info(f"text_chunks 表写入成功: {len(chunks)} 个切片, hash={f_hash}")
```

**改进点：**
- ✅ 分别删除每个表的旧记录（而不是一次性删除所有表）
- ✅ 先确保 files 表写入成功，再处理检索表
- ✅ 每个表的操作都有独立的错误处理和日志

### 3. 添加 file_hash 验证

**修改位置：** `etl.py:175-177`

```python
f_hash = calculate_file_hash(local_path)
if not f_hash:
    return {"success": False, "msg": "文件hash计算失败", "count": 0, "status": "error"}
```

**改进点：**
- ✅ 确保 file_hash 不为空才继续处理

### 4. 新增数据诊断功能

**修改位置：** `app.py` 新增 Tab5 "🔧 数据诊断"

功能包括：
- ✅ 统计各表的记录数和唯一 file_hash 数量
- ✅ 检查 text/image 表中的 file_hash 是否都在 files 表中存在
- ✅ 显示缺失的 file_hash 和对应的文件名
- ✅ 一键清理孤立记录（谨慎使用）

## 🚀 使用指南

### 1. 重新上传文件（推荐）

对于已经出现"未找到原始文件"错误的文件：

1. 找到原始文件
2. 在"📥 数据接入"页面重新上传
3. 系统会自动检测重复（通过 file_hash）
4. 覆盖旧数据，确保 files 表有记录

**注意**：新代码会在 files 表写入失败时立即报错，不会产生不一致的数据。

### 2. 使用数据诊断工具

1. 进入"🔧 数据诊断"标签页
2. 点击"🔍 诊断数据一致性"按钮
3. 查看诊断结果：
   - 各表的记录数统计
   - file_hash 一致性检查
   - 缺失的 file_hash 列表

4. 根据诊断结果选择修复方案：
   - **方案1（推荐）**：重新上传原始文件
   - **方案2**：点击"🗑️ 清理孤立记录"删除不一致的数据

### 3. 查看日志排查问题

如果上传失败，查看日志文件：

```bash
# 查看日志文件
cat app.log | grep "files 表"

# 查看最近的错误
cat app.log | grep "ERROR" | tail -20
```

日志会显示：
- files 表写入是否成功
- 失败的具体原因（网络、权限、大小等）
- 完整的异常堆栈信息

### 4. 检查 S3 配置

如果频繁出现 files 表写入失败，检查 `config.py` 中的 S3 配置：

```python
S3_CONFIG = {
    "endpoint_url": "http://192.168.20.4:8333",  # ← 确保可访问
    "aws_access_key_id": "mykey",                # ← 确保正确
    "aws_secret_access_key": "mysecret",         # ← 确保正确
    "bucket_name": "demo-bucket",                # ← 确保存在
}
```

测试 S3 连接：
```python
from etl import get_s3_client
client = get_s3_client()
if client:
    print("S3 连接成功")
    # 列出 bucket
    response = client.list_buckets()
    print("可用的 buckets:", [b['Name'] for b in response['Buckets']])
else:
    print("S3 连接失败")
```

## 🔧 故障排查清单

### 问题：上传时报错 "files表写入失败"

**可能原因：**
1. S3 连接失败
2. LanceDB 权限问题
3. 文件过大导致超时
4. 磁盘空间不足

**排查步骤：**
1. 检查 S3 配置和网络连接
2. 查看 `app.log` 中的详细错误信息
3. 尝试上传小文件测试
4. 检查磁盘空间：`df -h`

### 问题：检索时报错 "未找到原始文件"

**可能原因：**
1. 文件是用旧版本代码上传的（files 表写入失败但未报错）
2. file_hash 不匹配
3. files 表记录被意外删除

**解决方案：**
1. 使用"🔧 数据诊断"工具检查一致性
2. 重新上传原始文件
3. 或清理孤立记录

### 问题：覆盖上传后仍然找不到文件

**可能原因：**
1. 新上传也失败了（检查日志）
2. file_hash 计算不一致

**解决方案：**
1. 查看上传时的日志，确认是否成功
2. 使用诊断工具检查 files 表是否有记录
3. 尝试删除旧记录后重新上传

## 📊 数据流程图

```
上传文件
  ↓
计算 file_hash
  ↓
检查是否重复 (SQLite file_registry)
  ↓
读取文件 bytes
  ↓
【关键】写入 files 表 (LanceDB)
  ├─ 成功 → 继续
  └─ 失败 → 立即返回错误 ❌
  ↓
提取内容（文本/图像）
  ↓
向量化
  ↓
写入 text_chunks / image_chunks 表
  ↓
完成 ✅

检索流程
  ↓
在 text_chunks / image_chunks 表中搜索
  ↓
获取结果中的 file_hash
  ↓
在 files 表中查询 file_hash
  ├─ 找到 → 读取 file_bytes 预览/下载 ✅
  └─ 未找到 → 显示错误 ❌
```

## 💡 最佳实践

1. **上传前检查**：确保 S3 连接正常
2. **定期诊断**：使用诊断工具检查数据一致性
3. **查看日志**：上传失败时立即查看日志
4. **小批量测试**：大批量上传前先测试几个文件
5. **备份重要文件**：保留原始文件以便重新上传

## 📝 总结

**核心改进：**
- ✅ files 表写入失败时立即报错，不再产生不一致数据
- ✅ 改进覆盖模式逻辑，避免数据丢失
- ✅ 添加详细的日志记录，便于排查问题
- ✅ 提供数据诊断工具，快速发现和修复问题

**对于已有的问题数据：**
- 使用"🔧 数据诊断"工具检查
- 重新上传原始文件（推荐）
- 或清理孤立记录

**对于新上传的文件：**
- 如果 files 表写入失败，会立即看到错误提示
- 不会再出现"检索能找到但预览找不到"的情况
